/home/yuanhang/anaconda3/envs/prompt/lib/python3.8/site-packages/transformers/
------------------------------------------------------------------------------------------------------------------------
Task1: mnli/qqp ------------------------------------------------------------
# train bi
python -u nlp_main.py -d mnli --model_class QAClassifierModel --composition pooler --label_num 2 --train_batch_size 128 --val_batch_size 128 --no_apex  -n 3 | tee  logs/qqp/bi.log

# train poly
python -u nlp_main.py -d mnli --model_class PolyEncoder --label_num 3 --train_batch_size 128 --gradient_accumulation_steps 1 --one_stage  --model_save_prefix c64_ --no_apex -n 0 --context_num 64 | tee  logs/mnli/c64_poly.log

# train cross
python -u nlp_main.py -d mnli --model_class CrossBERT --composition pooler --label_num 2 --train_batch_size 16 --gradient_accumulation_steps 8  -n 1  | tee  logs/qqp/cross.log

# train ClassifyParallelEncoder
python -u nlp_main.py -d mnli --model_class ClassifyParallelEncoder --label_num 3 --train_batch_size 64 --gradient_accumulation_steps 2  --one_stage  --no_apex  --model_save_prefix c1_ablation_  --context_num 1 -n 0 --do_ablation | tee -a logs/mnli/c1_ablation_mine.log

# train Deformer
python -u nlp_main.py -d mnli --no_apex --model_class ClassifyDeformer --top_layer_num 3 --first_seq_max_len 256 -n 1 --label_num 3 --train_batch_size 16 --gradient_accumulation_steps 8 --one_stage --pretrained_bert_path bert-base-uncased | tee -a logs/mnli/deformer_c12.log


------------------------------------------------------------------------------------------------------------------------
Task2: ubuntu ------------------------------------------------------------
python -u nlp_main.py -d ubuntu --val_candidate_num 10  --model_class MatchParallelEncoder --one_stage --label_num 0 --no_apex  --val_batch_size 20 --train_batch_size 48 --gradient_accumulation_steps 1 --context_num 1 -n 0 --seed 100 --model_save_prefix s100_c1_ | tee logs/ubuntu/s00_c1_mine.log

python -u nlp_main.py -d ubuntu --val_candidate_num 10  --model_class PolyEncoder --composition pooler -n 0 --label_num 0 --train_batch_size 48  --val_batch_size 2  --context_num 16 | tee  -a logs/ubuntu/poly_16.log

python -u nlp_main.py --val_candidate_num 10  -d ubuntu --model_class MatchParallelEncoder --composition pooler -n 1 --label_num 0 --train_batch_size 48 --no_apex  --val_batch_size 4  --context_num 10 --no_initial_test | tee -a logs/ubuntu/c10_my.log

python -u nlp_main.py --label_num 1 --val_candidate_num 10  -d ubuntu --model_class MatchDeformer --no_apex  --val_batch_size 4  -n 0  --train_batch_size 2  --no_initial_test | tee -a logs/ubuntu/deformer.log

python -u nlp_main.py  -d ubuntu --model_class MatchDeformer --label_num 1 --val_candidate_num 10 --top_layer_num 2 --first_seq_max_len 256 -n 0 --train_batch_size 2 --gradient_accumulation_steps 24  --no_apex  --val_batch_size 4  --no_initial_test | tee logs/ubuntu/deformer.log

python -u nlp_main.py -d ubuntu --no_apex --model_class CrossBERT --label_num 1 --train_candidate_num 16 --val_candidate_num 10 --val_batch_size 2 --train_batch_size 4 --gradient_accumulation_steps 32  -n 1 --no_initial_test | tee logs/ubuntu/cross_bs_128.log

************* efficient  training ***********************
python -u nlp_main.py -d ubuntu --val_candidate_num 10  --model_class MatchParallelEncoder --one_stage --label_num 0 --no_apex  --val_batch_size 20 --train_batch_size 48  --pretrained_bert_path bert-base-uncased --context_num 1 -n 1 --seed 42 --model_save_prefix s42_c1_base_ --text_max_len 256 --step_max_query_num 48 --no_initial_test | tee logs/ubuntu/s42_c1_base_mine.log

python -u nlp_main.py -d ubuntu --val_candidate_num 10  --model_class QAMatchModel --one_stage --label_num 0 --no_apex  --val_batch_size 20 --train_batch_size 48  --pretrained_bert_path bert-base-uncased  -n 0 --seed 42 --model_save_prefix s42_base_ --text_max_len 256 --step_max_query_num 48  | tee logs/ubuntu/s42_base_bi.log

************* real test ***********************
1. PolyEncoder
python -u nlp_main.py -d ubuntu --model_class PolyEncoder --val_candidate_num 10 --one_stage --label_num 0 --train_batch_size 128  --val_batch_size 2 -n 1  --no_train --do_real_test --query_block_size 100 --context_num 16
2. Deformer
python -u nlp_main.py -d ubuntu --model_class MatchDeformer --val_candidate_num 10 --one_stage --label_num 1 --train_batch_size 128  --val_batch_size 2 -n 1  --no_train --do_real_test --query_block_size 100 --context_num 16 --top_layer_num 2

------------------------------------------------------------------------------------------------------------------------
Task2: dstc7 ------------------------------------------------------------
# train cross
python -u nlp_main.py -d dstc7 --model_class CrossBERT --composition pooler -n 0 --label_num 1 --train_candidate_num 16 --train_batch_size 2 --gradient_accumulation_steps 64 --val_batch_size 2 --no_initial_test | tee  logs/dstc7/cross_candidate_16.log

# train bi.
python -u nlp_main.py -d dstc7 --model_class QAMatchModel --composition pooler -n 1 --label_num 0 --train_batch_size 48  --val_batch_size 2 --no_apex | tee  -a logs/ubuntu/bi.log

# train mine. gradient_accumulation_steps must be 1!!!!!!!!
python -u nlp_main.py -d dstc7 --model_class MatchParallelEncoder --val_candidate_num 100 --one_stage --label_num 0 --train_batch_size 128  --val_batch_size 2 -n 0 --context_num 3  --model_save_prefix c3_one_stage_out_vector_128_ --no_initial_test | tee -a logs/dstc7/mine_context3_size_128.log

python -u nlp_main.py -d dstc7 --model_class PolyEncoder -n 0 --label_num 0 --train_batch_size 48  --val_batch_size 2 --one_stage --model_save_prefix c16_ --context_num 16 --no_initial_test | tee -a logs/dstc7/poly_encoder_candidate_16.log

# 进行测速
python -u nlp_main.py -d dstc7 --model_class MatchParallelEncoder --val_candidate_num 100 --one_stage --label_num 0 --train_batch_size 128  --val_batch_size 2 -n 1 --context_num 1 --model_save_prefix c3_one_stage_out_vector_128_ --no_train --do_real_test --query_block_size 100

python -u nlp_main.py -d dstc7 --model_class CrossBERT --val_candidate_num 100 --one_stage --label_num 1 --train_candidate_num 16 --train_batch_size 2  --gradient_accumulation_steps 64 --val_batch_size 2 -n 1 --no_train --do_real_test

*********** efficient  training ***********************
python -u nlp_main.py -d dstc7  --model_class PolyEncoder --one_stage --label_num 0 --no_apex  --val_batch_size 2 --train_batch_size 48  --pretrained_bert_path bert-base-uncased --context_num 16 -n 1 --seed 100 --model_save_prefix s100_c16_base_ --text_max_len 256 --step_max_query_num 24 --no_initial_test |  tee logs/dstc7/s100_c16_base_poly.log

------------------------------------------------------------------------------------------------------------------------
Task4: yahooqa ------------------------------------------------------------
1. Mine
python -u nlp_main.py -d yahooqa --model_class MatchParallelEncoder --label_num 0  --train_batch_size 32 --gradient_accumulation_steps 4 --one_stage  --model_save_prefix c10_ -n 0 --context_num 10 --no_apex  --train_candidate_num 5 | tee -a logs/yahooqa/c10_l4.log

2. bi
python -u nlp_main.py -d yahooqa --model_class QAMatchModel --label_num 0  --train_batch_size 32 --gradient_accumulation_steps 4 --one_stage  -n 1 --no_apex  --train_candidate_num 5 | tee -a logs/yahooqa/bi_l4.log

3. Cross
python -u nlp_main.py -d yahooqa --model_class CrossBERT --label_num 1 --train_candidate_num 5 --val_candidate_num 5 --val_batch_size 2 --no_apex --train_batch_size 32 --gradient_accumulation_steps 4  --seed 100 --model_save_prefix s100_ -n 0  | tee logs/yahooqa/s100_cross_bs_128.log
