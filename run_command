# 同时进行预训练和训练
python -u main.py -d ask_ubuntu -m 50 --model_class InputMemorySelfAtt -n 0 --mlm --model_class InputMemorySelfAtt --model_save_prefix mlm_m50_ask_ubuntu_ --train_batch_size 16 --gradient_accumulation_steps 4  --load_memory --memory_save_prefix layer4_ | tee logs/ask_ubuntu/mlm_m50_input_memory.log

# 读取预训练的memory进行训练
python -u main.py -d ask_ubuntu -m 50 --model_class InputMemorySelfAtt -n 1 --model_class InputMemorySelfAtt --model_save_prefix fast_mlm_m50_ask_ubuntu_ --train_batch_size 16 --gradient_accumulation_steps 4  --load_memory --memory_save_prefix layer4_ | tee logs/ask_ubuntu/fast_mlm_m50_input_memory.log

# 读取memory，进行一阶段训练，验证下经过预训练的memory是否还需要二阶段
python -u main.py -d ask_ubuntu -m 50 --model_class InputMemorySelfAtt -n 1 --model_class InputMemorySelfAtt --model_save_prefix one_stage_mlm_m50_ask_ubuntu_ --train_batch_size 16 --gradient_accumulation_steps 4  --load_memory --memory_save_prefix new_layer4_ | tee logs/ask_ubuntu/one_stage_mlm_m50_input_memory.log

# 输入为TBA，最普通的训练
python -u main.py -d so_python -m 50 --model_class InputMemorySelfAtt -n 1 --model_save_prefix one_stage_m50_ --train_batch_size 16 --gradient_accumulation_steps 4 | tee -a logs/so_python/one_stage_m50_input_memory.log

# 输入为TBA，在最后一阶段被中断了，要恢复训练
python -u main.py -d super_user -m 75 --model_class InputMemorySelfAtt -n 1 --model_save_prefix two_stage_m75_ --train_batch_size 16 --gradient_accumulation_steps 4 --restore --only_final | tee -a logs/super_user/two_stage_m75_input_memory.log

# 输入为qa，加上memory的最普通训练(pooler)
python -u main.py -d ask_ubuntu -m 50 --model_class QAMemory -n 1 --model_save_prefix m50_ --train_batch_size 16 --gradient_accumulation_steps 4  | tee -a logs/ask_ubuntu/two_stage_m50_qa_input_memory.log

# 输入为qa，不加上memory的最普通训练(pooler)
python -u main.py -d ask_ubuntu --model_class QAModel -n 1 --train_batch_size 16 --gradient_accumulation_steps 4  | tee -a logs/ask_ubuntu/two_stage_qa_input.log

# cross model
python -u main.py -d ask_ubuntu -m 50 --model_class CrossBERT -n 1 --train_batch_size 16 --gradient_accumulation_steps 4 | tee -a logs/ask_ubuntu/CrossBERT.log

# 半升级版，只有Q和A，没有H
python -u main.py -d ask_ubuntu -m 50 --model_class ADecoder -n 0 --train_batch_size 16 --gradient_accumulation_steps 4 | tee   logs/ask_ubuntu/ADecoder_v2.0.log